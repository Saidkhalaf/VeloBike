{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "import ConnectionConfig as cc\n",
    "cc.setupEnvironment()\n",
    "spark = cc.startLocalCluster(\"DeltaTableEx\")\n",
    "spark.getActiveSession()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-05T18:08:14.268244Z",
     "start_time": "2024-10-05T18:08:11.856555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x20c7f5df6d0>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DeltaTableEx</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "from delta import DeltaTable\n",
    "\n",
    "# Step 1: Load the dataset into a Delta table\n",
    "transaction_data_path = \"./FileStore/tables/transactions.csv\"  # Replace with the actual path to the transaction data CSV file\n",
    "transaction_delta_path = \"./spark-warehouse/transaction_data_delta\"  # Replace with the actual path where you want to store the Delta table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-05T18:08:14.285469Z",
     "start_time": "2024-10-05T18:08:14.280794Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Persist a deltatable to disk based on the CSV transactions.csv\n",
    "Read the CSV file transaction_data_path\n",
    "Write it as a Delta table to transaction_delta_path\n",
    "Use inferSchema option (see: https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/) to automatically infer the schema from the CSV file. Otherwise all columns will be of type string"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-05T18:08:14.309180Z",
     "start_time": "2024-10-05T18:08:14.304733Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a DeltaTable object for the persisted transaction data\n",
    "Use DetlaTable.forPath() to create a DeltaTable object for the persisted transaction data\n",
    "A delta table object is needed to use functions like history() and vacuum()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-05T18:08:14.323494Z",
     "start_time": "2024-10-05T18:08:14.320862Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the schema and history information for the Delta table\n",
    "To get the schema get the dataframe object with toDf() and use printSchema()\n",
    "Delta table has a history() method that returns a dataframe with the history of the delta table\n",
    "Make sure you understand the results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-05T18:08:14.350382Z",
     "start_time": "2024-10-05T18:08:14.347348Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a query on the delta table to find the total number of transactions\n",
    "Use toDF() to convert the delta table to a spark dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-05T18:08:14.364525Z",
     "start_time": "2024-10-05T18:08:14.361426Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create a view on the delta table with the name 'transactions'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-05T18:08:14.383791Z",
     "start_time": "2024-10-05T18:08:14.380967Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Update the delta table to increase the quantity of a specific product by a given value\n",
    "You canw write an update statement on the delta table with spark.sql(). This is not supported without the use of delta table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-05T18:08:14.488253Z",
     "start_time": "2024-10-05T18:08:14.483115Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Consult the history of the Delta table again\n",
    "Try to understand what you see\n",
    "When the results are truncated, you can use the vertical option in method show() to see the full results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-05T18:08:14.514474Z",
     "start_time": "2024-10-05T18:08:14.509767Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Delete all transactions from customer 101\n",
    "You canw write an delete statement on the delta table with spark.sql(). This is not supported without the use of delta table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-05T18:08:14.552680Z",
     "start_time": "2024-10-05T18:08:14.547436Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Consult the history of the Delta table again to see what is changed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-05T18:08:14.571648Z",
     "start_time": "2024-10-05T18:08:14.568042Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Perform an merge operation\n",
    "Get info on merge operations: https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge&language-sql\n",
    "1. Create a new dataframe original_df that reads the original transaction data from the CSV file and create a temporary view 'original_transactions'\n",
    "2. Perform a merge of  'original_transactions' into 'transactions'\n",
    "3. When a match is found, update the quantity for that row to 0\n",
    "4. When no match is found, insert the row from the new dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-05T18:08:14.600133Z",
     "start_time": "2024-10-05T18:08:14.593706Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Show the detla table and detla table history\n",
    "Is the result what you expected?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-05T18:08:14.624254Z",
     "start_time": "2024-10-05T18:08:14.620063Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Perform a select on the delta table on the second version\n",
    "Use option versionAsOf when reading (spark.read) the delta table from disk\n",
    "You can also travel back in time with option timestampAsOf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-10-05T18:08:14.681641Z",
     "start_time": "2024-10-05T18:08:14.678693Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
