{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2024-10-05T13:47:41.354498Z",
     "start_time": "2024-10-05T13:47:29.720216Z"
=======
     "end_time": "2024-10-05T18:16:01.359590Z",
     "start_time": "2024-10-05T18:15:55.142104Z"
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "import ConnectionConfig as cc\n",
    "cc.setupEnvironment()\n",
    "spark = cc.startLocalCluster(\"SQLExcercise\")\n",
    "spark.getActiveSession()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "<pyspark.sql.session.SparkSession at 0x17e714b95d0>"
=======
       "<pyspark.sql.session.SparkSession at 0x283877afd10>"
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
<<<<<<< HEAD
       "            <p><a href=\"http://192.168.0.219:4042\">Spark UI</a></p>\n",
=======
       "            <p><a href=\"http://host.docker.internal:4044\">Spark UI</a></p>\n",
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SQLExcercise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
<<<<<<< HEAD
     "execution_count": 1,
=======
     "execution_count": 16,
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
<<<<<<< HEAD
   "execution_count": 1
=======
   "execution_count": 16
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
  },
  {
   "cell_type": "markdown",
   "source": [
    "Go to https://spark.apache.org/docs/latest/sql-getting-started.html and https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Quickstart:-DataFrame to get some insights in coding Spark SQL. Always select 'Python' as the language.\n",
    "\n",
    "Use the Spark SQL Reference documentation to complete this excercise\n",
    "- To write dataframe operations: Python SparkSQL API: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html\n",
    "- To write pure SQL statements: Spark SQL API: https://spark.apache.org/docs/2.3.0/api/sql/index.html and https://spark.apache.org/docs/latest/sql-ref.html\n",
    "Helpfull site with examples: https://sparkbyexamples.com/pyspark/\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load employees.csv as a Spark Dataframe\n",
    "Tip: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Getting-Data-In/Out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Extract \n",
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .load(\"./FileStore/tables/employees.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2024-10-05T13:47:46.534991Z",
     "start_time": "2024-10-05T13:47:41.373042Z"
    }
   },
   "outputs": [],
   "execution_count": 2
=======
     "end_time": "2024-10-05T18:16:02.431108Z",
     "start_time": "2024-10-05T18:16:01.373951Z"
    }
   },
   "outputs": [],
   "execution_count": 17
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Display the schema of the DataFrame\n",
    "Tip: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Getting-Data-In/Out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.printSchema()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2024-10-05T13:47:48.360324Z",
     "start_time": "2024-10-05T13:47:48.351976Z"
=======
     "end_time": "2024-10-05T18:16:02.447516Z",
     "start_time": "2024-10-05T18:16:02.442209Z"
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
<<<<<<< HEAD
   "execution_count": 3
=======
   "execution_count": 18
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a temperary view of the dataset with name tbl_employees\n",
    "https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Getting-Data-In/Out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.createOrReplaceTempView(\"tbl_employees\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2024-10-05T13:47:48.550028Z",
     "start_time": "2024-10-05T13:47:48.507466Z"
    }
   },
   "outputs": [],
   "execution_count": 4
=======
     "end_time": "2024-10-05T18:16:02.488924Z",
     "start_time": "2024-10-05T18:16:02.476685Z"
    }
   },
   "outputs": [],
   "execution_count": 19
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Calculate the total number of employees in two ways:\n",
    "-   Via dataframe operations: Tip: https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html#Grouping-Data\n",
    "-   With a sql statement op tbl_employees: use spark.sql()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.groupBy().count().show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2024-10-05T13:47:49.330837Z",
     "start_time": "2024-10-05T13:47:48.565077Z"
=======
     "end_time": "2024-10-05T18:16:02.655215Z",
     "start_time": "2024-10-05T18:16:02.495440Z"
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   10|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
<<<<<<< HEAD
   "execution_count": 5
=======
   "execution_count": 20
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
  },
  {
   "cell_type": "code",
   "source": [
    "spark.sql(\"select count(employee_id) from tbl_employees\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2024-10-05T13:47:49.892909Z",
     "start_time": "2024-10-05T13:47:49.388199Z"
=======
     "end_time": "2024-10-05T18:16:02.853189Z",
     "start_time": "2024-10-05T18:16:02.701745Z"
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|count(employee_id)|\n",
      "+------------------+\n",
      "|                10|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
<<<<<<< HEAD
   "execution_count": 6
=======
   "execution_count": 21
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find the average salary of all employees in two ways:\n",
    "-   Via the dataframe operation 'select'\n",
    "-   With a sql statement ont tbl_employees"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.select(avg(\"salary\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2024-10-05T13:47:50.303004Z",
     "start_time": "2024-10-05T13:47:49.948047Z"
=======
     "end_time": "2024-10-05T18:16:03.093666Z",
     "start_time": "2024-10-05T18:16:02.921635Z"
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     4820.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
<<<<<<< HEAD
   "execution_count": 7
=======
   "execution_count": 22
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
  },
  {
   "cell_type": "code",
   "source": [
    "spark.sql(\"select avg(salary) from tbl_employees\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2024-10-05T13:47:50.546997Z",
     "start_time": "2024-10-05T13:47:50.357400Z"
=======
     "end_time": "2024-10-05T18:16:03.270274Z",
     "start_time": "2024-10-05T18:16:03.116770Z"
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     4820.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
<<<<<<< HEAD
   "execution_count": 8
=======
   "execution_count": 23
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the explain plan of the sql statement\n",
    "1. use the method explain(mode=\"extended\") on the spark.sql statement and look  at the different plans Spark created to excecute the query.\n",
    "2. Read the physical plan from bottom to top and try to match the plan with the query you wrote. (Exchange means that the data is being shuffled between the executors)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spark.sql(\"select avg(salary) from tbl_employees\").explain(mode=\"extended\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2024-10-05T13:47:50.659371Z",
     "start_time": "2024-10-05T13:47:50.593086Z"
=======
     "end_time": "2024-10-05T18:16:03.319920Z",
     "start_time": "2024-10-05T18:16:03.295909Z"
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('avg('salary), None)]\n",
      "+- 'UnresolvedRelation [tbl_employees], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "avg(salary): double\n",
      "Aggregate [avg(salary#1216) AS avg(salary)#1287]\n",
      "+- SubqueryAlias tbl_employees\n",
      "   +- View (`tbl_employees`, [employee_id#1213,name#1214,department#1215,salary#1216,hire_date#1217])\n",
      "      +- Relation [employee_id#1213,name#1214,department#1215,salary#1216,hire_date#1217] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [avg(salary#1216) AS avg(salary)#1287]\n",
      "+- Project [salary#1216]\n",
      "   +- Relation [employee_id#1213,name#1214,department#1215,salary#1216,hire_date#1217] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
<<<<<<< HEAD
      "+- HashAggregate(keys=[], functions=[avg(salary#20)], output=[avg(salary)#91])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=159]\n",
      "      +- HashAggregate(keys=[], functions=[partial_avg(salary#20)], output=[sum#95, count#96L])\n",
      "         +- FileScan csv [salary#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/KDG 2024-25/Datawarehouse/FileStore/tables/employees.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<salary:int>\n",
=======
      "+- HashAggregate(keys=[], functions=[avg(salary#1216)], output=[avg(salary)#1287])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=820]\n",
      "      +- HashAggregate(keys=[], functions=[partial_avg(salary#1216)], output=[sum#1291, count#1292L])\n",
      "         +- FileScan csv [salary#1216] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/saidk/Data Analytics/Datawarehouse/FileStore/tables/emp..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<salary:int>\n",
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
      "\n"
     ]
    }
   ],
<<<<<<< HEAD
   "execution_count": 9
=======
   "execution_count": 24
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Go to the SparkUI in the tab SQL/Dataframe\n",
    "1. Search for the query plans in the SparkUI SQL tab.\n",
    "2. Try to understand the excution plan.\n",
    "3. Go to https://dzone.com/articles/debugging-spark-performance-using-explain-plan to get some insights in the operators of the plan. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find the highest salary in each department in two ways:\n",
    "-  Via the dataframe operation 'groupBy'\n",
    "-  With a sql statement ont tbl_employees"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import max\n",
    "highestSalariesByDepartment = df.groupBy(\"department\").agg(max(\"salary\").alias(\"highest_salary\"))\n",
    "highestSalariesByDepartment.show()\n",
    "\n",
    "spark.sql(f'select department, max(salary) as highest_salary from tbl_employees group by department' ).explain(mode=\"extended\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2024-10-05T13:47:51.606393Z",
     "start_time": "2024-10-05T13:47:50.879078Z"
=======
     "end_time": "2024-10-05T18:16:03.720085Z",
     "start_time": "2024-10-05T18:16:03.383092Z"
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "| department|highest_salary|\n",
      "+-----------+--------------+\n",
      "|         HR|          4800|\n",
      "|  Marketing|          4300|\n",
      "|Engineering|          6000|\n",
      "+-----------+--------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['department], ['department, 'max('salary) AS highest_salary#1317]\n",
      "+- 'UnresolvedRelation [tbl_employees], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "department: string, highest_salary: int\n",
      "Aggregate [department#1215], [department#1215, max(salary#1216) AS highest_salary#1317]\n",
      "+- SubqueryAlias tbl_employees\n",
      "   +- View (`tbl_employees`, [employee_id#1213,name#1214,department#1215,salary#1216,hire_date#1217])\n",
      "      +- Relation [employee_id#1213,name#1214,department#1215,salary#1216,hire_date#1217] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [department#1215], [department#1215, max(salary#1216) AS highest_salary#1317]\n",
      "+- Project [department#1215, salary#1216]\n",
      "   +- Relation [employee_id#1213,name#1214,department#1215,salary#1216,hire_date#1217] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
<<<<<<< HEAD
      "+- HashAggregate(keys=[department#19], functions=[max(salary#20)], output=[department#19, highest_salary#121])\n",
      "   +- Exchange hashpartitioning(department#19, 4), ENSURE_REQUIREMENTS, [plan_id=213]\n",
      "      +- HashAggregate(keys=[department#19], functions=[partial_max(salary#20)], output=[department#19, max#126])\n",
      "         +- FileScan csv [department#19,salary#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/KDG 2024-25/Datawarehouse/FileStore/tables/employees.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<department:string,salary:int>\n",
=======
      "+- HashAggregate(keys=[department#1215], functions=[max(salary#1216)], output=[department#1215, highest_salary#1317])\n",
      "   +- Exchange hashpartitioning(department#1215, 4), ENSURE_REQUIREMENTS, [plan_id=874]\n",
      "      +- HashAggregate(keys=[department#1215], functions=[partial_max(salary#1216)], output=[department#1215, max#1322])\n",
      "         +- FileScan csv [department#1215,salary#1216] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/saidk/Data Analytics/Datawarehouse/FileStore/tables/emp..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<department:string,salary:int>\n",
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
      "\n"
     ]
    }
   ],
<<<<<<< HEAD
   "execution_count": 10
=======
   "execution_count": 25
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate the total salary expenditure for each year"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "totalSalaryExpenditureByYear = df.groupBy(year(\"hire_date\").alias(\"year\")).agg(sum(\"salary\").alias(\"total_salary_expenditure\"))\n",
    "totalSalaryExpenditureByYear.show()\n",
    "\n",
    "spark.sql(f\"select year(hire_date), sum(salary) as total_salary_expenditure from tbl_employees group by year(hire_date)\").explain(mode=\"extended\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2024-10-05T13:47:52.173742Z",
     "start_time": "2024-10-05T13:47:51.641275Z"
=======
     "end_time": "2024-10-05T18:16:04.050325Z",
     "start_time": "2024-10-05T18:16:03.742986Z"
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+\n",
      "|year|total_salary_expenditure|\n",
      "+----+------------------------+\n",
      "|2019|                    8800|\n",
      "|2021|                   10300|\n",
      "|2020|                    9200|\n",
      "|2022|                    8700|\n",
      "|2018|                    6000|\n",
      "|2023|                    5200|\n",
      "+----+------------------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['year('hire_date)], [unresolvedalias('year('hire_date), None), 'sum('salary) AS total_salary_expenditure#1349]\n",
      "+- 'UnresolvedRelation [tbl_employees], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "year(hire_date): int, total_salary_expenditure: bigint\n",
      "Aggregate [year(hire_date#1217)], [year(hire_date#1217) AS year(hire_date)#1351, sum(salary#1216) AS total_salary_expenditure#1349L]\n",
      "+- SubqueryAlias tbl_employees\n",
      "   +- View (`tbl_employees`, [employee_id#1213,name#1214,department#1215,salary#1216,hire_date#1217])\n",
      "      +- Relation [employee_id#1213,name#1214,department#1215,salary#1216,hire_date#1217] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [_groupingexpression#1354], [_groupingexpression#1354 AS year(hire_date)#1351, sum(salary#1216) AS total_salary_expenditure#1349L]\n",
      "+- Project [salary#1216, year(hire_date#1217) AS _groupingexpression#1354]\n",
      "   +- Relation [employee_id#1213,name#1214,department#1215,salary#1216,hire_date#1217] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
<<<<<<< HEAD
      "+- HashAggregate(keys=[_groupingexpression#158], functions=[sum(salary#20)], output=[year(hire_date)#155, total_salary_expenditure#153L])\n",
      "   +- Exchange hashpartitioning(_groupingexpression#158, 4), ENSURE_REQUIREMENTS, [plan_id=276]\n",
      "      +- HashAggregate(keys=[_groupingexpression#158], functions=[partial_sum(salary#20)], output=[_groupingexpression#158, sum#160L])\n",
      "         +- Project [salary#20, year(hire_date#21) AS _groupingexpression#158]\n",
      "            +- FileScan csv [salary#20,hire_date#21] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/KDG 2024-25/Datawarehouse/FileStore/tables/employees.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<salary:int,hire_date:date>\n",
=======
      "+- HashAggregate(keys=[_groupingexpression#1354], functions=[sum(salary#1216)], output=[year(hire_date)#1351, total_salary_expenditure#1349L])\n",
      "   +- Exchange hashpartitioning(_groupingexpression#1354, 4), ENSURE_REQUIREMENTS, [plan_id=937]\n",
      "      +- HashAggregate(keys=[_groupingexpression#1354], functions=[partial_sum(salary#1216)], output=[_groupingexpression#1354, sum#1356L])\n",
      "         +- Project [salary#1216, year(hire_date#1217) AS _groupingexpression#1354]\n",
      "            +- FileScan csv [salary#1216,hire_date#1217] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/saidk/Data Analytics/Datawarehouse/FileStore/tables/emp..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<salary:int,hire_date:date>\n",
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
      "\n"
     ]
    }
   ],
<<<<<<< HEAD
   "execution_count": 11
=======
   "execution_count": 26
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate the number of employees per postal code\n",
    "Postal codes are available in the parquet file empPostalCodes\n",
    "Create a view for the parquet file and join the two datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
<<<<<<< HEAD
=======
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T18:16:04.302906Z",
     "start_time": "2024-10-05T18:16:04.073072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_PC =spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./FileStore/tables/empPostalCodes.csv\")\n",
    "df_PC.write.format(\"parquet\").mode(\"overwrite\").save(\"./FileStore/tables/empPostalCodes\")"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
   "cell_type": "code",
   "source": [
    "df_PC =spark.read.format(\"parquet\").load(\"./FileStore/tables/empPostalCodes\")\n",
    "df_PC.createOrReplaceTempView(\"tbl_empPostalCodes\")\n",
    "df_empPerPc = spark.sql(\"select p.postal_code, count(e.employee_id) as number_of_employees from tbl_employees e inner join tbl_empPostalCodes p on e.employee_id = p.emp_id group by postal_code\")\n",
    "df_empPerPc.explain(mode=\"extended\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2024-10-05T13:47:53.237722Z",
     "start_time": "2024-10-05T13:47:52.201265Z"
=======
     "end_time": "2024-10-05T18:16:04.414742Z",
     "start_time": "2024-10-05T18:16:04.309917Z"
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
    }
   },
   "outputs": [
    {
<<<<<<< HEAD
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/C:/KDG 2024-25/Datawarehouse/FileStore/tables/empPostalCodes.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df_PC \u001B[38;5;241m=\u001B[39m\u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparquet\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m./FileStore/tables/empPostalCodes\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m df_PC\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtbl_empPostalCodes\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m df_empPerPc \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mselect p.postal_code, count(e.employee_id) as number_of_employees from tbl_employees e inner join tbl_empPostalCodes p on e.employee_id = p.emp_id group by postal_code\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mC:\\KDG 2024-25\\Data Analytics\\sparkdelta\\venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:307\u001B[0m, in \u001B[0;36mDataFrameReader.load\u001B[1;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[0;32m    306\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 307\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    308\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    309\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(path) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n",
      "File \u001B[1;32mC:\\KDG 2024-25\\Data Analytics\\sparkdelta\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mC:\\KDG 2024-25\\Data Analytics\\sparkdelta\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: file:/C:/KDG 2024-25/Datawarehouse/FileStore/tables/empPostalCodes."
     ]
    }
   ],
   "execution_count": 12
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['postal_code], ['p.postal_code, 'count('e.employee_id) AS number_of_employees#1386]\n",
      "+- 'Join Inner, ('e.employee_id = 'p.emp_id)\n",
      "   :- 'SubqueryAlias e\n",
      "   :  +- 'UnresolvedRelation [tbl_employees], [], false\n",
      "   +- 'SubqueryAlias p\n",
      "      +- 'UnresolvedRelation [tbl_empPostalCodes], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "postal_code: string, number_of_employees: bigint\n",
      "Aggregate [postal_code#1383], [postal_code#1383, count(employee_id#1213) AS number_of_employees#1386L]\n",
      "+- Join Inner, (employee_id#1213 = cast(emp_id#1382 as int))\n",
      "   :- SubqueryAlias e\n",
      "   :  +- SubqueryAlias tbl_employees\n",
      "   :     +- View (`tbl_employees`, [employee_id#1213,name#1214,department#1215,salary#1216,hire_date#1217])\n",
      "   :        +- Relation [employee_id#1213,name#1214,department#1215,salary#1216,hire_date#1217] csv\n",
      "   +- SubqueryAlias p\n",
      "      +- SubqueryAlias tbl_emppostalcodes\n",
      "         +- View (`tbl_empPostalCodes`, [emp_id#1382,postal_code#1383])\n",
      "            +- Relation [emp_id#1382,postal_code#1383] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [postal_code#1383], [postal_code#1383, count(employee_id#1213) AS number_of_employees#1386L]\n",
      "+- Project [employee_id#1213, postal_code#1383]\n",
      "   +- Join Inner, (employee_id#1213 = cast(emp_id#1382 as int))\n",
      "      :- Project [employee_id#1213]\n",
      "      :  +- Filter isnotnull(employee_id#1213)\n",
      "      :     +- Relation [employee_id#1213,name#1214,department#1215,salary#1216,hire_date#1217] csv\n",
      "      +- Filter isnotnull(emp_id#1382)\n",
      "         +- Relation [emp_id#1382,postal_code#1383] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[postal_code#1383], functions=[count(employee_id#1213)], output=[postal_code#1383, number_of_employees#1386L])\n",
      "   +- Exchange hashpartitioning(postal_code#1383, 4), ENSURE_REQUIREMENTS, [plan_id=1013]\n",
      "      +- HashAggregate(keys=[postal_code#1383], functions=[partial_count(employee_id#1213)], output=[postal_code#1383, count#1391L])\n",
      "         +- Project [employee_id#1213, postal_code#1383]\n",
      "            +- BroadcastHashJoin [employee_id#1213], [cast(emp_id#1382 as int)], Inner, BuildLeft, false\n",
      "               :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1008]\n",
      "               :  +- Filter isnotnull(employee_id#1213)\n",
      "               :     +- FileScan csv [employee_id#1213] Batched: false, DataFilters: [isnotnull(employee_id#1213)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/saidk/Data Analytics/Datawarehouse/FileStore/tables/emp..., PartitionFilters: [], PushedFilters: [IsNotNull(employee_id)], ReadSchema: struct<employee_id:int>\n",
      "               +- Filter isnotnull(emp_id#1382)\n",
      "                  +- FileScan parquet [emp_id#1382,postal_code#1383] Batched: true, DataFilters: [isnotnull(emp_id#1382)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/saidk/Data Analytics/Datawarehouse/FileStore/tables/emp..., PartitionFilters: [], PushedFilters: [IsNotNull(emp_id)], ReadSchema: struct<emp_id:string,postal_code:string>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Write the results to a DeltaTable in the spark-warehouse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_empPerPc.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"employeesPerPostalCode\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2024-10-05T13:47:53.251327Z",
     "start_time": "2024-09-10T09:39:35.068066600Z"
=======
     "end_time": "2024-10-05T18:16:07.338193Z",
     "start_time": "2024-10-05T18:16:04.436543Z"
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
    }
   },
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "outputs": [],
   "source": "# spark.stop()",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:53.300357300Z",
     "start_time": "2024-09-10T09:40:01.188513600Z"
=======
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T18:16:07.849789Z",
     "start_time": "2024-10-05T18:16:07.343250Z"
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
    }
   },
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stuff to create the excercises. Not part of the excercise"
   ],
   "metadata": {
    "collapsed": false
   }
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "#df_PC =spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./FileStore/tables/empPostalCodes.csv\")\n",
    "#df_PC.write.format(\"parquet\").mode(\"overwrite\").save(\"./FileStore/tables/empPostalCodes\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-05T13:47:53.301362300Z",
     "start_time": "2024-09-10T09:39:52.920174800Z"
    }
   }
=======
>>>>>>> aae81819d776e09352dde843c9e9630e91018fdd
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
